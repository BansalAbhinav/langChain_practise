{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b1Fe61XA69FR"
      },
      "source": [
        "## Wikipedia Retriever"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "tVlVrUuQ6cCC"
      },
      "outputs": [],
      "source": [
        "from langchain_community.retrievers import WikipediaRetriever"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "q5FUCNmX7ICr"
      },
      "outputs": [],
      "source": [
        "# Initialize the retriever (optional: set language and top_k)\n",
        "retriever = WikipediaRetriever(top_k_results=2, lang=\"en\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "i-8sH-w67K8J"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Define your query\n",
        "query = \"one piece anime vs naruto anime \"\n",
        "\n",
        "# Get relevant Wikipedia documents\n",
        "docs = retriever.invoke(query)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6mD3zUptQSQg",
        "outputId": "e4963f93-dce0-4d6e-c955-4499196aa0f5"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[Document(metadata={'title': 'China–India relations', 'summary': \"China and India maintained peaceful relations for thousands of years, but their relationship has varied since the Chinese Communist Party (CCP)'s victory in the Chinese Civil War in 1949 and the annexation of Tibet by the People's Republic of China. The two nations have sought economic cooperation with each other, while frequent border disputes and economic nationalism in both countries are major points of contention.\\nCultural and economic relations between China and India date back to ancient times. The Silk Road not only served as a major trade route between India and China, but is also credited for facilitating the spread of Buddhism from India to East Asia. During the 19th century, China was involved in a growing opium trade with the East India Company, which exported opium grown in India. During World War II, both British India and the Republic of China (ROC) played a crucial role in halting the progress of Imperial Japan. After India became independent in 1947, it established relations with the ROC. The modern Sino-Indian diplomatic relationship began in 1950, when India was among the first noncommunist countries to end formal relations with the Republic of China and recognise the PRC as the legitimate government of both Mainland China and Taiwan. China and India are two of the major regional powers in Asia, and are the two most populous countries and among the fastest growing major economies in the world.\\nGrowth in diplomatic and economic influence has increased the significance of their bilateral relationship. Between 2008 and 2021, China has been India's largest trading partner, and the two countries have also extended their strategic and military relations. However, conflict of interest leads to hostility. India has a large trade deficit that is favoured towards China. The two countries failed to resolve their border dispute and Indian media outlets have repeatedly reported Chinese military incursions into Indian territory. And relations between contemporary China and India have been characterised by border disputes, resulting in three military conflicts – the Sino-Indian War of 1962, the border clashes in Nathu La and Cho La in 1967, and the 1987 Sumdorong Chu standoff. Since the late 1980s, both countries have successfully rebuilt diplomatic and economic ties.\\nSince 2013, border disputes have reemerged to take centre stage in the two countries' mutual relations. In early 2018, the two armies got engaged in a standoff at the Doklam plateau along the disputed Bhutan-China border. Since summer 2020, armed standoffs and skirmishes at multiple locations along the entire Sino-Indian border escalated. A serious clash occurred in the Galwan Valley, resulting in the death of 20 Indian soldiers and many Chinese soldiers. Both countries have steadily established military infrastructure along border areas, including amidst the 2020 China–India skirmishes. Additionally, India remains wary about China's strong strategic bilateral relations with Pakistan, and China's relations to separatist groups in Northeast India, while China has expressed concerns about Indian military and economic activities in the disputed South China Sea as well as hosting of anti-China activity from Tibetan exiles. Today, the South Asian region is the premier site of intensified great power competition between China and India.\", 'source': 'https://en.wikipedia.org/wiki/China%E2%80%93India_relations'}, page_content='China and India maintained peaceful relations for thousands of years, but their relationship has varied since the Chinese Communist Party (CCP)\\'s victory in the Chinese Civil War in 1949 and the annexation of Tibet by the People\\'s Republic of China. The two nations have sought economic cooperation with each other, while frequent border disputes and economic nationalism in both countries are major points of contention.\\nCultural and economic relations between China and India date back to ancient times. The Silk Road not only served as a major trade route between India and China, but is also credited for facilitating the spread of Buddhism from India to East Asia. During the 19th century, China was involved in a growing opium trade with the East India Company, which exported opium grown in India. During World War II, both British India and the Republic of China (ROC) played a crucial role in halting the progress of Imperial Japan. After India became independent in 1947, it established relations with the ROC. The modern Sino-Indian diplomatic relationship began in 1950, when India was among the first noncommunist countries to end formal relations with the Republic of China and recognise the PRC as the legitimate government of both Mainland China and Taiwan. China and India are two of the major regional powers in Asia, and are the two most populous countries and among the fastest growing major economies in the world.\\nGrowth in diplomatic and economic influence has increased the significance of their bilateral relationship. Between 2008 and 2021, China has been India\\'s largest trading partner, and the two countries have also extended their strategic and military relations. However, conflict of interest leads to hostility. India has a large trade deficit that is favoured towards China. The two countries failed to resolve their border dispute and Indian media outlets have repeatedly reported Chinese military incursions into Indian territory. And relations between contemporary China and India have been characterised by border disputes, resulting in three military conflicts – the Sino-Indian War of 1962, the border clashes in Nathu La and Cho La in 1967, and the 1987 Sumdorong Chu standoff. Since the late 1980s, both countries have successfully rebuilt diplomatic and economic ties.\\nSince 2013, border disputes have reemerged to take centre stage in the two countries\\' mutual relations. In early 2018, the two armies got engaged in a standoff at the Doklam plateau along the disputed Bhutan-China border. Since summer 2020, armed standoffs and skirmishes at multiple locations along the entire Sino-Indian border escalated. A serious clash occurred in the Galwan Valley, resulting in the death of 20 Indian soldiers and many Chinese soldiers. Both countries have steadily established military infrastructure along border areas, including amidst the 2020 China–India skirmishes. Additionally, India remains wary about China\\'s strong strategic bilateral relations with Pakistan, and China\\'s relations to separatist groups in Northeast India, while China has expressed concerns about Indian military and economic activities in the disputed South China Sea as well as hosting of anti-China activity from Tibetan exiles. Today, the South Asian region is the premier site of intensified great power competition between China and India.\\n\\n\\n== Geopolitical overview ==\\n\\nThe China–India border straddles the Himalayas, with Nepal and Bhutan acting as buffer states. Parts of the disputed Kashmir region claimed by India (J&K and Ladakh) are claimed and administered by either Pakistan (Azad Kashmir and Gilgit Baltistan) or by the PRC (Aksai Chin). The Government of Pakistan, on its maps, shows the Aksai Chin area as mostly within China and labels the boundary \"Frontier Undefined\", while India holds that Aksai Chin is illegally occupied by the PRC. China and India also dispute most of Arunachal Pradesh.\\nNot only is China\\'s India policy shaped by greater competition with the'),\n",
              " Document(metadata={'title': 'Indo-Pakistani war of 1971', 'summary': \"The Indo-Pakistani war of 1971, also known as the third Indo-Pakistani war, was a military confrontation between India and Pakistan that occurred during the Bangladesh Liberation War in East Pakistan from 3 December 1971 until the Pakistani capitulation in Dhaka on 16 December 1971.  The war began with Pakistan's Operation Chengiz Khan, consisting of preemptive aerial strikes on eight Indian air stations. The strikes led to India declaring war on Pakistan, marking their entry into the war for East Pakistan's independence, on the side of Bengali nationalist forces. India's entry expanded the existing conflict with Indian and Pakistani forces engaging on both the eastern and western fronts. \\nThirteen days after the war started, India achieved a clear upper hand, and the Eastern Command of the Pakistan military signed the instrument of surrender on 16 December 1971 in Dhaka, marking the formation of East Pakistan as the new nation of Bangladesh. Approximately 93,000 Pakistani servicemen were taken prisoner by the Indian Army, which included 79,676 to 81,000 uniformed personnel of the Pakistan Armed Forces, including some Bengali soldiers who had remained loyal to Pakistan. The remaining 10,324 to 12,500 prisoners were civilians, either family members of the military personnel or collaborators (Razakars).\\nIt is estimated that members of the Pakistani military and supporting pro-Pakistani Islamist militias killed between 300,000 and 3,000,000 civilians in Bangladesh. As a result of the conflict, a further eight to ten million people fled the country to seek refuge in India.\\nDuring the war, members of the Pakistani military and supporting pro-Pakistani Islamist militias called the Razakars raped between 200,000 and 400,000 Bangladeshi women and girls in a systematic campaign of genocidal rape.\", 'source': 'https://en.wikipedia.org/wiki/Indo-Pakistani_war_of_1971'}, page_content=\"The Indo-Pakistani war of 1971, also known as the third Indo-Pakistani war, was a military confrontation between India and Pakistan that occurred during the Bangladesh Liberation War in East Pakistan from 3 December 1971 until the Pakistani capitulation in Dhaka on 16 December 1971.  The war began with Pakistan's Operation Chengiz Khan, consisting of preemptive aerial strikes on eight Indian air stations. The strikes led to India declaring war on Pakistan, marking their entry into the war for East Pakistan's independence, on the side of Bengali nationalist forces. India's entry expanded the existing conflict with Indian and Pakistani forces engaging on both the eastern and western fronts. \\nThirteen days after the war started, India achieved a clear upper hand, and the Eastern Command of the Pakistan military signed the instrument of surrender on 16 December 1971 in Dhaka, marking the formation of East Pakistan as the new nation of Bangladesh. Approximately 93,000 Pakistani servicemen were taken prisoner by the Indian Army, which included 79,676 to 81,000 uniformed personnel of the Pakistan Armed Forces, including some Bengali soldiers who had remained loyal to Pakistan. The remaining 10,324 to 12,500 prisoners were civilians, either family members of the military personnel or collaborators (Razakars).\\nIt is estimated that members of the Pakistani military and supporting pro-Pakistani Islamist militias killed between 300,000 and 3,000,000 civilians in Bangladesh. As a result of the conflict, a further eight to ten million people fled the country to seek refuge in India.\\nDuring the war, members of the Pakistani military and supporting pro-Pakistani Islamist militias called the Razakars raped between 200,000 and 400,000 Bangladeshi women and girls in a systematic campaign of genocidal rape.\\n\\n\\n== Background ==\\n\\nThe Indo-Pakistani conflict was sparked by the Bangladesh Liberation War, which was a result of the violation of the rights of East Pakistan by the Pakistan Army. The political tensions in East Pakistan had its origin in the creation of Pakistan as a result of the partition of India by the United Kingdom in 1947; the popular language movement in 1950; mass riots in East Bengal in 1964; and the mass protests in 1969. These led to the resignation of President Ayub Khan, who invited army chief General Yahya Khan to take over the central government. The geographical distance between the eastern and western wings of Pakistan was vast; East Pakistan lay over 1,600 kilometres (1,000 mi) away, which greatly hampered any attempt to integrate the Bengali and the Pakistani cultures.\\nTo overcome the Bengali domination and prevent formation of the central government in Islamabad, the controversial One Unit programme established the two wings of East and West Pakistan. West Pakistanis' opposition to these efforts made it difficult to effectively govern both wings. In 1969, President Yahya Khan announced the first general elections and disestablished the status of West Pakistan as a single province in 1970, in order to restore it to its original heterogeneous status comprising four provinces, as defined at the time of establishment of Pakistan in 1947. In addition, there were religious and racial tensions between Bengalis and the multi-ethnic West Pakistanis, as Bengalis looked different from the dominant West Pakistanis.\\nThe East Pakistan's Awami League leader Sheikh Mujibur Rahman stressed his political position by presenting his Six Points and endorsing the Bengalis' right to govern. The 1970 Pakistani general election, resulted in Awami League gaining 167 out of 169 seats for the East Pakistan Legislative Assembly, and a near-absolute majority in the 313-seat National Assembly, while the vote in West Pakistan was mostly won by the socialist Pakistan Peoples Party. The League's election success caused many West Pakistanis to fear that it would allow the Bengalis to draft the constitution based on the six-points and liberalism.\\nTo resol\")]"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "docs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UH3lPwDi7Myp",
        "outputId": "156011e5-ff17-4cfc-ec82-b7621f814f6f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "--- Result 1 ---\n",
            "Content:\n",
            "Naruto is a Japanese anime television series based on Masashi Kishimoto's 1999–2014 manga series Naruto. It follows Naruto Uzumaki, a young orphan ninja who seeks recognition from his peers and dreams of becoming the Hokage, the leader of the Village Hidden in the Leaves. Like the manga, the anime series is divided into two separate parts: the first series retains the original manga's title and is set in the world of ninjas. The second series, a direct sequel titled Naruto: Shippuden, takes place during his teens. Both anime series were animated by Pierrot, produced by Aniplex, and licensed by Viz Media in North America.\n",
            "The first anime series aired on TV Tokyo and ran for 220 episodes from October 2002 to February 2007. An English dub produced by Viz Media aired on Cartoon Network and YTV from September 2005 to December 2009. The second series, Naruto: Shippuden, also aired on TV Tokyo and ran for 500 episodes from February 2007 to March 2017. The English dub of Naruto: Shippuden was broadcast on Disney XD in the United States from October 2009 to November 2011, airing the first 98 episodes before eventually switching over to Adult Swim's Toonami programming block in January 2014 to September 2024, starting over from the first episode. After Disney XD removed the series from broadcast, Viz Media began streaming new English dubbed episodes on their streaming service Neon Alley in December 2012 starting at episode 99. The service aborted its run in March 2016 after 338 episodes due to its shutdown a month later. Besides the anime television series, Pierrot also developed 11 animated films and 12 original video animations.\n",
            "The anime series achieved significant commercial success, becoming one of Viz Media's top-earning franchise and being a cultural impact with the run of the series. It was the third most-watched series in the United States by 2020. Critically, it received mixed reception. Its adaptation of Kishimoto's art style and story pacing was not received well. The fight scenes, character dynamics, and emotional depth received critical acclaim. Naruto: Shippuden was consistently ranked as one of the most-watched in Japan. It was lauded for its improved animation, more mature tone, well-crafted character interactions, and balanced storytelling. The first anime ranked 38th in IGN's Top 100 Animated Series and Shippuden earned a nomination from the Crunchyroll Anime Awards for Best Continuing Series. Viz Media sold over three million anime home video units by 2019.\n",
            "\n",
            "\n",
            "== Series overview ==\n",
            "\n",
            "\n",
            "=== Part I ===\n",
            "\n",
            "A powerful fox known as the Nine-Tails attacks Konoha, the hidden leaf village in the Land of Fire, one of the Five Great Shinobi Countries in the Ninja World. In response, the leader of Konoha and the Fourth Hokage, Minato Namikaze, at the cost of his life, seals the fox inside the body of his newborn son, Naruto Uzumaki, making him a host of the beast. The Third Hokage returns from retirement to become the leader of Konoha again. Naruto is often scorned by Konoha's villagers for being the host of the Nine-Tails. Due to a decree by the Third Hokage forbidding any mention of these events, Naruto learns nothing about the Nine-Tails until 12 years later, when Mizuki, a renegade ninja, reveals the truth to him. Naruto defeats Mizuki in combat, earning the respect of his teacher, Iruka Umino.\n",
            "Shortly afterward, Naruto becomes a ninja and joins with Sasuke Uchiha, against whom he often competes, and Sakura Haruno, on whom he has a crush, to form Team 7, under an experienced sensei, the elite ninja Kakashi Hatake. Like all the ninja teams from every village, Team 7 completes missions requested by the villagers, ranging from doing chores and being bodyguards to performing assassinations.\n",
            "After several missions, including a major one in the Land of Waves, Kakashi allows Team 7 to take a ninja exam, enabling them to advance to a higher rank and take on more difficult missions, known as Chunin Exams. During the exams, Orochimaru, a ...\n",
            "\n",
            "--- Result 2 ---\n",
            "Content:\n",
            "Naruto Uzumaki (Japanese: うずまき ナルト, Hepburn: Uzumaki Naruto) () is the titular protagonist of the manga series Naruto, created by Masashi Kishimoto. He is a ninja from the fictional Hidden Leaf Village (Japanese: 木ノ葉隠れ, Hepburn: konohagakure). As a boy, Naruto is ridiculed and ostracized on account of the Nine-Tailed Demon Fox—a malevolent creature that attacked Konohagakure—that was sealed away in his body. Despite this, he aspires to become his village's leader, the Hokage, in order to receive their approval. His carefree, optimistic, and boisterous personality enables him to befriend other Konohagakure ninja, as well as ninja from other villages. Naruto appears in the series's films and in other media related to the franchise, including video games and original video animations (OVA), as well as the sequel Boruto: Naruto Next Generations, where he is the Hokage, and his son, Boruto Uzumaki, is the protagonist.\n",
            "When creating Naruto for the initial part of the series, Kishimoto kept the character \"simple and stupid\" while giving him many attributes of an ideal hero. Kishimoto gave Naruto a dark side by adding tragedy to the character's past. He has revised Naruto's image many times, providing the character with simple clothes to fit the young demography. Kishimoto changed his design for Part II of the storyline, which starts two-and-a-half years after Part I. Naruto is voiced by Junko Takeuchi in the original animated series and Maile Flanagan in the English adaptations.\n",
            "Merchandise based on Naruto includes figurines and keychains. Naruto's character development has been praised by anime and manga publications and has drawn scholarly attention. Although some initially saw him as a typical manga and anime protagonist comparable to those in other shōnen manga, others have praised his personality and character development as he avoids stereotypes typically seen in similar media. The character has also been the subject of research in literature, making him stand out in fiction based on his traits and growth.\n",
            "\n",
            "\n",
            "== Creation and conception ==\n",
            "\n",
            "\n",
            "=== Original concept and influences ===\n",
            "\n",
            "During the 1990s, new manga author Masashi Kishimoto sought to write a one-shot chapter that would feature Naruto as a chef, but this version never made it to print. Kishimoto originally wanted to make Naruto a child who could transform into a fox, so he created a one-shot of Naruto for the summer 1997 issue of Akamaru Jump magazine based on the idea. When comparing both the Naruto one-shot and his other work, Karakuri, Kishimoto realized that the former's title character was more appealing than the lead of Karakuri. Kishimoto reflects that Naruto's \"honest\" smile was well received in contrast to the sly look the main character from Karakuri had. Following the success of another one-shot, Mario, Kishimoto started working on the Naruto series, where he wanted to reuse the title character from his earlier one-shot. Kishimoto wrote the first two chapters to show his appeal to the readers and then focus on the other protagonists despite difficulties. Following the second chapter, Kishimoto introduced the other protagonists, but with bad relationships, including with Sasuke Uchiha and Naruto's constant rejected affections for Sakura Haruno. The manga story was planned to show Naruto's coming-of-age through multiple fights, and he looked forward to seeing the conclusion.\n",
            "For the serialized version, Kishimoto incorporated traits he felt made an ideal hero in the creation of Naruto: a straightforward way of thinking, a mischievous side, and attributes possessed by Goku from the Dragon Ball franchise, aiming to keep Naruto \"simple and stupid\". Although Goku was a major influence to Naruto, Kishimoto was more attracted by Dragon Ball character Krillin, as he comes across as more human than the protagonist for displaying flaws that made the readers easier to accept in a similar fashion to his mentor Iruka Umino. Kishimoto avoided modeling him after anyone in part...\n"
          ]
        }
      ],
      "source": [
        "# Print retrieved content\n",
        "for i, doc in enumerate(docs):\n",
        "    print(f\"\\n--- Result {i+1} ---\")\n",
        "    print(f\"Content:\\n{doc.page_content}...\")  # truncate for display"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z2qt9J157djt"
      },
      "source": [
        "## Vector Store Retriever"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "sUKzoti97OKc"
      },
      "outputs": [],
      "source": [
        "from langchain_community.vectorstores import Chroma\n",
        "from langchain_huggingface import ChatHuggingFace , HuggingFaceEndpoint , HuggingFaceEmbeddings\n",
        "from langchain_core.documents import Document"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "dLymTTcA7nbZ"
      },
      "outputs": [],
      "source": [
        "# Step 1: Your source documents\n",
        "documents = [\n",
        "    Document(page_content=\"LangChain helps developers build LLM applications easily.\"),\n",
        "    Document(page_content=\"Chroma is a vector database optimized for LLM-based search.\"),\n",
        "    Document(page_content=\"Embeddings convert text into high-dimensional vectors.\"),\n",
        "    Document(page_content=\"OpenAI provides powerful embedding models.\"),\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "B5J0HYsr7prG"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "d:\\Coding\\langchain_Models\\venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        }
      ],
      "source": [
        "# Step 2: Initialize embedding model\n",
        "embedding_model = HuggingFaceEmbeddings()\n",
        "\n",
        "# Step 3: Create Chroma vector store in memory\n",
        "vectorstore = Chroma.from_documents(\n",
        "    documents=documents,\n",
        "    embedding=embedding_model,\n",
        "    collection_name=\"my_collection\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "xvSgoUYs7sLu"
      },
      "outputs": [],
      "source": [
        "# Step 4: Convert vectorstore into a retriever\n",
        "retriever = vectorstore.as_retriever(search_kwargs={\"k\": 2})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "I5l0rXl67v4N"
      },
      "outputs": [],
      "source": [
        "query = \"What is Chroma used for?\"\n",
        "results = retriever.invoke(query)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OMAEeCxK7yRR",
        "outputId": "7f140c90-bf2d-4d54-8988-813b74ed281b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "--- Result 1 ---\n",
            "Chroma is a vector database optimized for LLM-based search.\n",
            "\n",
            "--- Result 2 ---\n",
            "Embeddings convert text into high-dimensional vectors.\n"
          ]
        }
      ],
      "source": [
        "for i, doc in enumerate(results):\n",
        "    print(f\"\\n--- Result {i+1} ---\")\n",
        "    print(doc.page_content)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "hh0zC7L6R1Zn"
      },
      "outputs": [],
      "source": [
        "results = vectorstore.similarity_search(query, k=2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J7kYMau5R1sC",
        "outputId": "d7b05617-45d7-4584-d18a-a578e4a28e89"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "--- Result 1 ---\n",
            "Chroma is a vector database optimized for LLM-based search.\n",
            "\n",
            "--- Result 2 ---\n",
            "Embeddings convert text into high-dimensional vectors.\n"
          ]
        }
      ],
      "source": [
        "for i, doc in enumerate(results):\n",
        "    print(f\"\\n--- Result {i+1} ---\")\n",
        "    print(doc.page_content)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zDwJT1x9A2tI"
      },
      "source": [
        "## MMR"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "8uL8bET570ud"
      },
      "outputs": [],
      "source": [
        "# Sample documents\n",
        "docs = [\n",
        "    Document(page_content=\"LangChain makes it easy to work with LLMs.\"),\n",
        "    Document(page_content=\"LangChain is used to build LLM based applications.\"),\n",
        "    Document(page_content=\"Chroma is used to store and search document embeddings.\"),\n",
        "    Document(page_content=\"Embeddings are vector representations of text.\"),\n",
        "    Document(page_content=\"MMR helps you get diverse results when doing similarity search.\"),\n",
        "    Document(page_content=\"LangChain supports Chroma, FAISS, Pinecone, and more.\"),\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "TM7GvqKeA8ml"
      },
      "outputs": [],
      "source": [
        "from langchain_community.vectorstores import FAISS\n",
        "\n",
        "# Initialize OpenAI embeddings\n",
        "embedding_model = HuggingFaceEmbeddings()\n",
        "\n",
        "# Step 2: Create the FAISS vector store from documents\n",
        "vectorstore = FAISS.from_documents(\n",
        "    documents=docs,\n",
        "    embedding=embedding_model\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ueDg6SApBAqt"
      },
      "outputs": [],
      "source": [
        "# Enable MMR in the retriever\n",
        "retriever = vectorstore.as_retriever(\n",
        "    search_type=\"mmr\",                   # <-- This enables MMR\n",
        "    search_kwargs={\"k\": 3, \"lambda_mult\": 0.2}  # k = top results, lambda_mult = relevance-diversity balance least value more diverse result  between 0 and 1 more towards 1 more simple output\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "52AkKNz7Chcf"
      },
      "outputs": [],
      "source": [
        "query = \"What is langchain?\"\n",
        "results = retriever.invoke(query)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H-HvthKyCkBL",
        "outputId": "6bec200b-9541-480f-fa94-1b8c6b3174f2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "--- Result 1 ---\n",
            "LangChain is used to build LLM based applications.\n",
            "\n",
            "--- Result 2 ---\n",
            "Embeddings are vector representations of text.\n",
            "\n",
            "--- Result 3 ---\n",
            "MMR helps you get diverse results when doing similarity search.\n"
          ]
        }
      ],
      "source": [
        "for i, doc in enumerate(results):\n",
        "    print(f\"\\n--- Result {i+1} ---\")\n",
        "    print(doc.page_content)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HFybBaL-Eu8-"
      },
      "source": [
        "## Multiquery Retriever"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "Djm-59wI1J-s"
      },
      "outputs": [],
      "source": [
        "from langchain_community.vectorstores import FAISS\n",
        "from langchain_openai import OpenAIEmbeddings\n",
        "from langchain_core.documents import Document\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langchain.retrievers.multi_query import MultiQueryRetriever"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "Vxr_cxaJ1KQV"
      },
      "outputs": [],
      "source": [
        "# Relevant health & wellness documents\n",
        "all_docs = [\n",
        "    Document(page_content=\"Regular walking boosts heart health and can reduce symptoms of depression.\", metadata={\"source\": \"H1\"}),\n",
        "    Document(page_content=\"Consuming leafy greens and fruits helps detox the body and improve longevity.\", metadata={\"source\": \"H2\"}),\n",
        "    Document(page_content=\"Deep sleep is crucial for cellular repair and emotional regulation.\", metadata={\"source\": \"H3\"}),\n",
        "    Document(page_content=\"Mindfulness and controlled breathing lower cortisol and improve mental clarity.\", metadata={\"source\": \"H4\"}),\n",
        "    Document(page_content=\"Drinking sufficient water throughout the day helps maintain metabolism and energy.\", metadata={\"source\": \"H5\"}),\n",
        "    Document(page_content=\"The solar energy system in modern homes helps balance electricity demand.\", metadata={\"source\": \"I1\"}),\n",
        "    Document(page_content=\"Python balances readability with power, making it a popular system design language.\", metadata={\"source\": \"I2\"}),\n",
        "    Document(page_content=\"Photosynthesis enables plants to produce energy by converting sunlight.\", metadata={\"source\": \"I3\"}),\n",
        "    Document(page_content=\"The 2022 FIFA World Cup was held in Qatar and drew global energy and excitement.\", metadata={\"source\": \"I4\"}),\n",
        "    Document(page_content=\"Black holes bend spacetime and store immense gravitational energy.\", metadata={\"source\": \"I5\"}),\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "pq6eouD41KfF"
      },
      "outputs": [],
      "source": [
        "# Initialize OpenAI embeddings\n",
        "embedding_model = HuggingFaceEmbeddings()\n",
        "\n",
        "# Create FAISS vector store\n",
        "vectorstore = FAISS.from_documents(documents=all_docs, embedding=embedding_model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "kh62ZNvD1Krv"
      },
      "outputs": [],
      "source": [
        "# Create retrievers\n",
        "similarity_retriever = vectorstore.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 5})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "id": "vXUHDW6m1K75"
      },
      "outputs": [],
      "source": [
        "multiquery_retriever = MultiQueryRetriever.from_llm(\n",
        "    retriever=vectorstore.as_retriever(search_kwargs={\"k\": 5}),\n",
        "    llm= HuggingFaceEndpoint(\n",
        "    repo_id=\"google/gemma-2-2b-it\",\n",
        "    task=\"text-generation\"\n",
        ")\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "id": "9ybExIQQ1bnt"
      },
      "outputs": [],
      "source": [
        "# Query\n",
        "query = \"How to improve energy levels and maintain balance?\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "id": "V2SlmAoi1gOP"
      },
      "outputs": [
        {
          "ename": "ValueError",
          "evalue": "Model google/gemma-2-2b-it is not supported for task text-generation and provider nebius. Supported task: conversational.",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[44]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Retrieve results\u001b[39;00m\n\u001b[32m      2\u001b[39m similarity_results = similarity_retriever.invoke(query)\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m multiquery_results= \u001b[43mmultiquery_retriever\u001b[49m\u001b[43m.\u001b[49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32md:\\Coding\\langchain_Models\\venv\\Lib\\site-packages\\langchain_core\\retrievers.py:261\u001b[39m, in \u001b[36mBaseRetriever.invoke\u001b[39m\u001b[34m(self, input, config, **kwargs)\u001b[39m\n\u001b[32m    259\u001b[39m kwargs_ = kwargs \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._expects_other_args \u001b[38;5;28;01melse\u001b[39;00m {}\n\u001b[32m    260\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._new_arg_supported:\n\u001b[32m--> \u001b[39m\u001b[32m261\u001b[39m     result = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_get_relevant_documents\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    262\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs_\u001b[49m\n\u001b[32m    263\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    264\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    265\u001b[39m     result = \u001b[38;5;28mself\u001b[39m._get_relevant_documents(\u001b[38;5;28minput\u001b[39m, **kwargs_)\n",
            "\u001b[36mFile \u001b[39m\u001b[32md:\\Coding\\langchain_Models\\venv\\Lib\\site-packages\\langchain\\retrievers\\multi_query.py:172\u001b[39m, in \u001b[36mMultiQueryRetriever._get_relevant_documents\u001b[39m\u001b[34m(self, query, run_manager)\u001b[39m\n\u001b[32m    158\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_get_relevant_documents\u001b[39m(\n\u001b[32m    159\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    160\u001b[39m     query: \u001b[38;5;28mstr\u001b[39m,\n\u001b[32m    161\u001b[39m     *,\n\u001b[32m    162\u001b[39m     run_manager: CallbackManagerForRetrieverRun,\n\u001b[32m    163\u001b[39m ) -> \u001b[38;5;28mlist\u001b[39m[Document]:\n\u001b[32m    164\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Get relevant documents given a user query.\u001b[39;00m\n\u001b[32m    165\u001b[39m \n\u001b[32m    166\u001b[39m \u001b[33;03m    Args:\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    170\u001b[39m \u001b[33;03m        Unique union of relevant documents from all generated queries\u001b[39;00m\n\u001b[32m    171\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m172\u001b[39m     queries = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgenerate_queries\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    173\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.include_original:\n\u001b[32m    174\u001b[39m         queries.append(query)\n",
            "\u001b[36mFile \u001b[39m\u001b[32md:\\Coding\\langchain_Models\\venv\\Lib\\site-packages\\langchain\\retrievers\\multi_query.py:191\u001b[39m, in \u001b[36mMultiQueryRetriever.generate_queries\u001b[39m\u001b[34m(self, question, run_manager)\u001b[39m\n\u001b[32m    178\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mgenerate_queries\u001b[39m(\n\u001b[32m    179\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    180\u001b[39m     question: \u001b[38;5;28mstr\u001b[39m,\n\u001b[32m    181\u001b[39m     run_manager: CallbackManagerForRetrieverRun,\n\u001b[32m    182\u001b[39m ) -> \u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mstr\u001b[39m]:\n\u001b[32m    183\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Generate queries based upon user input.\u001b[39;00m\n\u001b[32m    184\u001b[39m \n\u001b[32m    185\u001b[39m \u001b[33;03m    Args:\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    189\u001b[39m \u001b[33;03m        List of LLM generated queries that are similar to the user input\u001b[39;00m\n\u001b[32m    190\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m191\u001b[39m     response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mllm_chain\u001b[49m\u001b[43m.\u001b[49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    192\u001b[39m \u001b[43m        \u001b[49m\u001b[43m{\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mquestion\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mquestion\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    193\u001b[39m \u001b[43m        \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m=\u001b[49m\u001b[43m{\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcallbacks\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_child\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    194\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    195\u001b[39m     lines = response[\u001b[33m\"\u001b[39m\u001b[33mtext\u001b[39m\u001b[33m\"\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m.llm_chain, LLMChain) \u001b[38;5;28;01melse\u001b[39;00m response\n\u001b[32m    196\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.verbose:\n",
            "\u001b[36mFile \u001b[39m\u001b[32md:\\Coding\\langchain_Models\\venv\\Lib\\site-packages\\langchain_core\\runnables\\base.py:3082\u001b[39m, in \u001b[36mRunnableSequence.invoke\u001b[39m\u001b[34m(self, input, config, **kwargs)\u001b[39m\n\u001b[32m   3080\u001b[39m                 input_ = context.run(step.invoke, input_, config, **kwargs)\n\u001b[32m   3081\u001b[39m             \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m3082\u001b[39m                 input_ = \u001b[43mcontext\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m.\u001b[49m\u001b[43minvoke\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3083\u001b[39m \u001b[38;5;66;03m# finish the root run\u001b[39;00m\n\u001b[32m   3084\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
            "\u001b[36mFile \u001b[39m\u001b[32md:\\Coding\\langchain_Models\\venv\\Lib\\site-packages\\langchain_core\\language_models\\llms.py:389\u001b[39m, in \u001b[36mBaseLLM.invoke\u001b[39m\u001b[34m(self, input, config, stop, **kwargs)\u001b[39m\n\u001b[32m    378\u001b[39m \u001b[38;5;129m@override\u001b[39m\n\u001b[32m    379\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34minvoke\u001b[39m(\n\u001b[32m    380\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    385\u001b[39m     **kwargs: Any,\n\u001b[32m    386\u001b[39m ) -> \u001b[38;5;28mstr\u001b[39m:\n\u001b[32m    387\u001b[39m     config = ensure_config(config)\n\u001b[32m    388\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m (\n\u001b[32m--> \u001b[39m\u001b[32m389\u001b[39m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgenerate_prompt\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    390\u001b[39m \u001b[43m            \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_convert_input\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    391\u001b[39m \u001b[43m            \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    392\u001b[39m \u001b[43m            \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcallbacks\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    393\u001b[39m \u001b[43m            \u001b[49m\u001b[43mtags\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtags\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    394\u001b[39m \u001b[43m            \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmetadata\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    395\u001b[39m \u001b[43m            \u001b[49m\u001b[43mrun_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mrun_name\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    396\u001b[39m \u001b[43m            \u001b[49m\u001b[43mrun_id\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpop\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mrun_id\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    397\u001b[39m \u001b[43m            \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    398\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    399\u001b[39m         .generations[\u001b[32m0\u001b[39m][\u001b[32m0\u001b[39m]\n\u001b[32m    400\u001b[39m         .text\n\u001b[32m    401\u001b[39m     )\n",
            "\u001b[36mFile \u001b[39m\u001b[32md:\\Coding\\langchain_Models\\venv\\Lib\\site-packages\\langchain_core\\language_models\\llms.py:766\u001b[39m, in \u001b[36mBaseLLM.generate_prompt\u001b[39m\u001b[34m(self, prompts, stop, callbacks, **kwargs)\u001b[39m\n\u001b[32m    757\u001b[39m \u001b[38;5;129m@override\u001b[39m\n\u001b[32m    758\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mgenerate_prompt\u001b[39m(\n\u001b[32m    759\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    763\u001b[39m     **kwargs: Any,\n\u001b[32m    764\u001b[39m ) -> LLMResult:\n\u001b[32m    765\u001b[39m     prompt_strings = [p.to_string() \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m prompts]\n\u001b[32m--> \u001b[39m\u001b[32m766\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt_strings\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32md:\\Coding\\langchain_Models\\venv\\Lib\\site-packages\\langchain_core\\language_models\\llms.py:971\u001b[39m, in \u001b[36mBaseLLM.generate\u001b[39m\u001b[34m(self, prompts, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\u001b[39m\n\u001b[32m    956\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\u001b[38;5;28mself\u001b[39m.cache \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m get_llm_cache() \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m.cache \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m:\n\u001b[32m    957\u001b[39m     run_managers = [\n\u001b[32m    958\u001b[39m         callback_manager.on_llm_start(\n\u001b[32m    959\u001b[39m             \u001b[38;5;28mself\u001b[39m._serialized,\n\u001b[32m   (...)\u001b[39m\u001b[32m    969\u001b[39m         )\n\u001b[32m    970\u001b[39m     ]\n\u001b[32m--> \u001b[39m\u001b[32m971\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_generate_helper\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    972\u001b[39m \u001b[43m        \u001b[49m\u001b[43mprompts\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    973\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    974\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrun_managers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    975\u001b[39m \u001b[43m        \u001b[49m\u001b[43mnew_arg_supported\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mbool\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mnew_arg_supported\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    976\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    977\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    978\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(missing_prompts) > \u001b[32m0\u001b[39m:\n\u001b[32m    979\u001b[39m     run_managers = [\n\u001b[32m    980\u001b[39m         callback_managers[idx].on_llm_start(\n\u001b[32m    981\u001b[39m             \u001b[38;5;28mself\u001b[39m._serialized,\n\u001b[32m   (...)\u001b[39m\u001b[32m    988\u001b[39m         \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m missing_prompt_idxs\n\u001b[32m    989\u001b[39m     ]\n",
            "\u001b[36mFile \u001b[39m\u001b[32md:\\Coding\\langchain_Models\\venv\\Lib\\site-packages\\langchain_core\\language_models\\llms.py:792\u001b[39m, in \u001b[36mBaseLLM._generate_helper\u001b[39m\u001b[34m(self, prompts, stop, run_managers, new_arg_supported, **kwargs)\u001b[39m\n\u001b[32m    781\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_generate_helper\u001b[39m(\n\u001b[32m    782\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    783\u001b[39m     prompts: \u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mstr\u001b[39m],\n\u001b[32m   (...)\u001b[39m\u001b[32m    788\u001b[39m     **kwargs: Any,\n\u001b[32m    789\u001b[39m ) -> LLMResult:\n\u001b[32m    790\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    791\u001b[39m         output = (\n\u001b[32m--> \u001b[39m\u001b[32m792\u001b[39m             \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_generate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    793\u001b[39m \u001b[43m                \u001b[49m\u001b[43mprompts\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    794\u001b[39m \u001b[43m                \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    795\u001b[39m \u001b[43m                \u001b[49m\u001b[38;5;66;43;03m# TODO: support multiple run managers\u001b[39;49;00m\n\u001b[32m    796\u001b[39m \u001b[43m                \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrun_managers\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrun_managers\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    797\u001b[39m \u001b[43m                \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    798\u001b[39m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    799\u001b[39m             \u001b[38;5;28;01mif\u001b[39;00m new_arg_supported\n\u001b[32m    800\u001b[39m             \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m._generate(prompts, stop=stop)\n\u001b[32m    801\u001b[39m         )\n\u001b[32m    802\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    803\u001b[39m         \u001b[38;5;28;01mfor\u001b[39;00m run_manager \u001b[38;5;129;01min\u001b[39;00m run_managers:\n",
            "\u001b[36mFile \u001b[39m\u001b[32md:\\Coding\\langchain_Models\\venv\\Lib\\site-packages\\langchain_core\\language_models\\llms.py:1545\u001b[39m, in \u001b[36mLLM._generate\u001b[39m\u001b[34m(self, prompts, stop, run_manager, **kwargs)\u001b[39m\n\u001b[32m   1542\u001b[39m new_arg_supported = inspect.signature(\u001b[38;5;28mself\u001b[39m._call).parameters.get(\u001b[33m\"\u001b[39m\u001b[33mrun_manager\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m   1543\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m prompt \u001b[38;5;129;01min\u001b[39;00m prompts:\n\u001b[32m   1544\u001b[39m     text = (\n\u001b[32m-> \u001b[39m\u001b[32m1545\u001b[39m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1546\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m new_arg_supported\n\u001b[32m   1547\u001b[39m         \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m._call(prompt, stop=stop, **kwargs)\n\u001b[32m   1548\u001b[39m     )\n\u001b[32m   1549\u001b[39m     generations.append([Generation(text=text)])\n\u001b[32m   1550\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m LLMResult(generations=generations)\n",
            "\u001b[36mFile \u001b[39m\u001b[32md:\\Coding\\langchain_Models\\venv\\Lib\\site-packages\\langchain_huggingface\\llms\\huggingface_endpoint.py:318\u001b[39m, in \u001b[36mHuggingFaceEndpoint._call\u001b[39m\u001b[34m(self, prompt, stop, run_manager, **kwargs)\u001b[39m\n\u001b[32m    315\u001b[39m         completion += chunk.text\n\u001b[32m    316\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m completion\n\u001b[32m--> \u001b[39m\u001b[32m318\u001b[39m response_text = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mclient\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtext_generation\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    319\u001b[39m \u001b[43m    \u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    320\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    321\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43minvocation_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    322\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    324\u001b[39m \u001b[38;5;66;03m# Maybe the generation has stopped at one of the stop sequences:\u001b[39;00m\n\u001b[32m    325\u001b[39m \u001b[38;5;66;03m# then we remove this stop sequence from the end of the generated text\u001b[39;00m\n\u001b[32m    326\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m stop_seq \u001b[38;5;129;01min\u001b[39;00m invocation_params[\u001b[33m\"\u001b[39m\u001b[33mstop\u001b[39m\u001b[33m\"\u001b[39m]:\n",
            "\u001b[36mFile \u001b[39m\u001b[32md:\\Coding\\langchain_Models\\venv\\Lib\\site-packages\\huggingface_hub\\inference\\_client.py:2376\u001b[39m, in \u001b[36mInferenceClient.text_generation\u001b[39m\u001b[34m(self, prompt, details, stream, model, adapter_id, best_of, decoder_input_details, do_sample, frequency_penalty, grammar, max_new_tokens, repetition_penalty, return_full_text, seed, stop, stop_sequences, temperature, top_k, top_n_tokens, top_p, truncate, typical_p, watermark)\u001b[39m\n\u001b[32m   2374\u001b[39m model_id = model \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m.model\n\u001b[32m   2375\u001b[39m provider_helper = get_provider_helper(\u001b[38;5;28mself\u001b[39m.provider, task=\u001b[33m\"\u001b[39m\u001b[33mtext-generation\u001b[39m\u001b[33m\"\u001b[39m, model=model_id)\n\u001b[32m-> \u001b[39m\u001b[32m2376\u001b[39m request_parameters = \u001b[43mprovider_helper\u001b[49m\u001b[43m.\u001b[49m\u001b[43mprepare_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2377\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2378\u001b[39m \u001b[43m    \u001b[49m\u001b[43mparameters\u001b[49m\u001b[43m=\u001b[49m\u001b[43mparameters\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2379\u001b[39m \u001b[43m    \u001b[49m\u001b[43mextra_payload\u001b[49m\u001b[43m=\u001b[49m\u001b[43m{\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstream\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2380\u001b[39m \u001b[43m    \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2381\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2382\u001b[39m \u001b[43m    \u001b[49m\u001b[43mapi_key\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2383\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2385\u001b[39m \u001b[38;5;66;03m# Handle errors separately for more precise error messages\u001b[39;00m\n\u001b[32m   2386\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
            "\u001b[36mFile \u001b[39m\u001b[32md:\\Coding\\langchain_Models\\venv\\Lib\\site-packages\\huggingface_hub\\inference\\_providers\\_common.py:90\u001b[39m, in \u001b[36mTaskProviderHelper.prepare_request\u001b[39m\u001b[34m(self, inputs, parameters, headers, model, api_key, extra_payload)\u001b[39m\n\u001b[32m     87\u001b[39m api_key = \u001b[38;5;28mself\u001b[39m._prepare_api_key(api_key)\n\u001b[32m     89\u001b[39m \u001b[38;5;66;03m# mapped model from HF model ID\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m90\u001b[39m provider_mapping_info = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_prepare_mapping_info\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     92\u001b[39m \u001b[38;5;66;03m# default HF headers + user headers (to customize in subclasses)\u001b[39;00m\n\u001b[32m     93\u001b[39m headers = \u001b[38;5;28mself\u001b[39m._prepare_headers(headers, api_key)\n",
            "\u001b[36mFile \u001b[39m\u001b[32md:\\Coding\\langchain_Models\\venv\\Lib\\site-packages\\huggingface_hub\\inference\\_providers\\_common.py:159\u001b[39m, in \u001b[36mTaskProviderHelper._prepare_mapping_info\u001b[39m\u001b[34m(self, model)\u001b[39m\n\u001b[32m    156\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mModel \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m is not supported by provider \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m.provider\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    158\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m provider_mapping.task != \u001b[38;5;28mself\u001b[39m.task:\n\u001b[32m--> \u001b[39m\u001b[32m159\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    160\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mModel \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m is not supported for task \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m.task\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m and provider \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m.provider\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m. \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    161\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mSupported task: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mprovider_mapping.task\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    162\u001b[39m     )\n\u001b[32m    164\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m provider_mapping.status == \u001b[33m\"\u001b[39m\u001b[33mstaging\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m    165\u001b[39m     logger.warning(\n\u001b[32m    166\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mModel \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m is in staging mode for provider \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m.provider\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m. Meant for test purposes only.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    167\u001b[39m     )\n",
            "\u001b[31mValueError\u001b[39m: Model google/gemma-2-2b-it is not supported for task text-generation and provider nebius. Supported task: conversational."
          ]
        }
      ],
      "source": [
        "# Retrieve results\n",
        "similarity_results = similarity_retriever.invoke(query)\n",
        "multiquery_results= multiquery_retriever.invoke(query)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yQUtMLupwfe2",
        "outputId": "743431a3-5b0e-4f69-8af3-dec66a49f7ce"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "--- Result 1 ---\n",
            "Drinking sufficient water throughout the day helps maintain metabolism and energy.\n",
            "\n",
            "--- Result 2 ---\n",
            "Mindfulness and controlled breathing lower cortisol and improve mental clarity.\n",
            "\n",
            "--- Result 3 ---\n",
            "Regular walking boosts heart health and can reduce symptoms of depression.\n",
            "\n",
            "--- Result 4 ---\n",
            "Deep sleep is crucial for cellular repair and emotional regulation.\n",
            "\n",
            "--- Result 5 ---\n",
            "The solar energy system in modern homes helps balance electricity demand.\n",
            "******************************************************************************************************************************************************\n",
            "\n",
            "--- Result 1 ---\n",
            "Drinking sufficient water throughout the day helps maintain metabolism and energy.\n",
            "\n",
            "--- Result 2 ---\n",
            "Mindfulness and controlled breathing lower cortisol and improve mental clarity.\n",
            "\n",
            "--- Result 3 ---\n",
            "Regular walking boosts heart health and can reduce symptoms of depression.\n",
            "\n",
            "--- Result 4 ---\n",
            "Consuming leafy greens and fruits helps detox the body and improve longevity.\n",
            "\n",
            "--- Result 5 ---\n",
            "Deep sleep is crucial for cellular repair and emotional regulation.\n"
          ]
        }
      ],
      "source": [
        "for i, doc in enumerate(similarity_results):\n",
        "    print(f\"\\n--- Result {i+1} ---\")\n",
        "    print(doc.page_content)\n",
        "\n",
        "print(\"*\"*150)\n",
        "\n",
        "for i, doc in enumerate(multiquery_results):\n",
        "    print(f\"\\n--- Result {i+1} ---\")\n",
        "    print(doc.page_content)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6AP7o9-VJ0Zq"
      },
      "source": [
        "## ContextualCompressionRetriever"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "id": "RVl6n3HH6gp_"
      },
      "outputs": [],
      "source": [
        "from langchain_community.vectorstores import FAISS\n",
        "from langchain_openai import OpenAIEmbeddings, ChatOpenAI\n",
        "from langchain.retrievers.contextual_compression import ContextualCompressionRetriever\n",
        "from langchain.retrievers.document_compressors import LLMChainExtractor\n",
        "from langchain_core.documents import Document"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "id": "RmLD6aZB6g1k"
      },
      "outputs": [],
      "source": [
        "# Recreate the document objects from the previous data\n",
        "docs = [\n",
        "    Document(page_content=(\n",
        "        \"\"\"The Grand Canyon is one of the most visited natural wonders in the world.\n",
        "        Photosynthesis is the process by which green plants convert sunlight into energy.\n",
        "        Millions of tourists travel to see it every year. The rocks date back millions of years.\"\"\"\n",
        "    ), metadata={\"source\": \"Doc1\"}),\n",
        "\n",
        "    Document(page_content=(\n",
        "        \"\"\"In medieval Europe, castles were built primarily for defense.\n",
        "        The chlorophyll in plant cells captures sunlight during photosynthesis.\n",
        "        Knights wore armor made of metal. Siege weapons were often used to breach castle walls.\"\"\"\n",
        "    ), metadata={\"source\": \"Doc2\"}),\n",
        "\n",
        "    Document(page_content=(\n",
        "        \"\"\"Basketball was invented by Dr. James Naismith in the late 19th century.\n",
        "        It was originally played with a soccer ball and peach baskets. NBA is now a global league.\"\"\"\n",
        "    ), metadata={\"source\": \"Doc3\"}),\n",
        "\n",
        "    Document(page_content=(\n",
        "        \"\"\"The history of cinema began in the late 1800s. Silent films were the earliest form.\n",
        "        Thomas Edison was among the pioneers. Photosynthesis does not occur in animal cells.\n",
        "        Modern filmmaking involves complex CGI and sound design.\"\"\"\n",
        "    ), metadata={\"source\": \"Doc4\"})\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "id": "tgMNKHnv6hD8"
      },
      "outputs": [],
      "source": [
        "# Create a FAISS vector store from the documents\n",
        "embedding_model = HuggingFaceEmbeddings()\n",
        "vectorstore = FAISS.from_documents(docs, embedding_model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "id": "vlAqc8lQrz_B"
      },
      "outputs": [],
      "source": [
        "base_retriever = vectorstore.as_retriever(search_kwargs={\"k\": 5})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "id": "nHjUdgiS6hRK"
      },
      "outputs": [],
      "source": [
        "# Set up the compressor using an LLM\n",
        "llm= HuggingFaceEndpoint(\n",
        "    repo_id=\"google/gemma-2-2b-it\",\n",
        "    task=\"text-generation\"\n",
        ")\n",
        "compressor = LLMChainExtractor.from_llm(llm)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "id": "um5gKEMT6hdB"
      },
      "outputs": [],
      "source": [
        "# Create the contextual compression retriever\n",
        "compression_retriever = ContextualCompressionRetriever(\n",
        "    base_retriever=base_retriever,\n",
        "    base_compressor=compressor\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "id": "lmHGenTo6hp6"
      },
      "outputs": [
        {
          "ename": "ValueError",
          "evalue": "Model google/gemma-2-2b-it is not supported for task text-generation and provider nebius. Supported task: conversational.",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[55]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Query the retriever\u001b[39;00m\n\u001b[32m      2\u001b[39m query = \u001b[33m\"\u001b[39m\u001b[33mWhat is photosynthesis?\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m compressed_results = \u001b[43mcompression_retriever\u001b[49m\u001b[43m.\u001b[49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32md:\\Coding\\langchain_Models\\venv\\Lib\\site-packages\\langchain_core\\retrievers.py:261\u001b[39m, in \u001b[36mBaseRetriever.invoke\u001b[39m\u001b[34m(self, input, config, **kwargs)\u001b[39m\n\u001b[32m    259\u001b[39m kwargs_ = kwargs \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._expects_other_args \u001b[38;5;28;01melse\u001b[39;00m {}\n\u001b[32m    260\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._new_arg_supported:\n\u001b[32m--> \u001b[39m\u001b[32m261\u001b[39m     result = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_get_relevant_documents\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    262\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs_\u001b[49m\n\u001b[32m    263\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    264\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    265\u001b[39m     result = \u001b[38;5;28mself\u001b[39m._get_relevant_documents(\u001b[38;5;28minput\u001b[39m, **kwargs_)\n",
            "\u001b[36mFile \u001b[39m\u001b[32md:\\Coding\\langchain_Models\\venv\\Lib\\site-packages\\langchain\\retrievers\\contextual_compression.py:46\u001b[39m, in \u001b[36mContextualCompressionRetriever._get_relevant_documents\u001b[39m\u001b[34m(self, query, run_manager, **kwargs)\u001b[39m\n\u001b[32m     40\u001b[39m docs = \u001b[38;5;28mself\u001b[39m.base_retriever.invoke(\n\u001b[32m     41\u001b[39m     query,\n\u001b[32m     42\u001b[39m     config={\u001b[33m\"\u001b[39m\u001b[33mcallbacks\u001b[39m\u001b[33m\"\u001b[39m: run_manager.get_child()},\n\u001b[32m     43\u001b[39m     **kwargs,\n\u001b[32m     44\u001b[39m )\n\u001b[32m     45\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m docs:\n\u001b[32m---> \u001b[39m\u001b[32m46\u001b[39m     compressed_docs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbase_compressor\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcompress_documents\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     47\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdocs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     48\u001b[39m \u001b[43m        \u001b[49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     49\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_child\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     50\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     51\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(compressed_docs)\n\u001b[32m     52\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m []\n",
            "\u001b[36mFile \u001b[39m\u001b[32md:\\Coding\\langchain_Models\\venv\\Lib\\site-packages\\langchain\\retrievers\\document_compressors\\chain_extract.py:73\u001b[39m, in \u001b[36mLLMChainExtractor.compress_documents\u001b[39m\u001b[34m(self, documents, query, callbacks)\u001b[39m\n\u001b[32m     71\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m doc \u001b[38;5;129;01min\u001b[39;00m documents:\n\u001b[32m     72\u001b[39m     _input = \u001b[38;5;28mself\u001b[39m.get_input(query, doc)\n\u001b[32m---> \u001b[39m\u001b[32m73\u001b[39m     output_ = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mllm_chain\u001b[49m\u001b[43m.\u001b[49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_input\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m=\u001b[49m\u001b[43m{\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcallbacks\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     74\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m.llm_chain, LLMChain):\n\u001b[32m     75\u001b[39m         output = output_[\u001b[38;5;28mself\u001b[39m.llm_chain.output_key]\n",
            "\u001b[36mFile \u001b[39m\u001b[32md:\\Coding\\langchain_Models\\venv\\Lib\\site-packages\\langchain_core\\runnables\\base.py:3082\u001b[39m, in \u001b[36mRunnableSequence.invoke\u001b[39m\u001b[34m(self, input, config, **kwargs)\u001b[39m\n\u001b[32m   3080\u001b[39m                 input_ = context.run(step.invoke, input_, config, **kwargs)\n\u001b[32m   3081\u001b[39m             \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m3082\u001b[39m                 input_ = \u001b[43mcontext\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m.\u001b[49m\u001b[43minvoke\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3083\u001b[39m \u001b[38;5;66;03m# finish the root run\u001b[39;00m\n\u001b[32m   3084\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
            "\u001b[36mFile \u001b[39m\u001b[32md:\\Coding\\langchain_Models\\venv\\Lib\\site-packages\\langchain_core\\language_models\\llms.py:389\u001b[39m, in \u001b[36mBaseLLM.invoke\u001b[39m\u001b[34m(self, input, config, stop, **kwargs)\u001b[39m\n\u001b[32m    378\u001b[39m \u001b[38;5;129m@override\u001b[39m\n\u001b[32m    379\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34minvoke\u001b[39m(\n\u001b[32m    380\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    385\u001b[39m     **kwargs: Any,\n\u001b[32m    386\u001b[39m ) -> \u001b[38;5;28mstr\u001b[39m:\n\u001b[32m    387\u001b[39m     config = ensure_config(config)\n\u001b[32m    388\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m (\n\u001b[32m--> \u001b[39m\u001b[32m389\u001b[39m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgenerate_prompt\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    390\u001b[39m \u001b[43m            \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_convert_input\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    391\u001b[39m \u001b[43m            \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    392\u001b[39m \u001b[43m            \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcallbacks\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    393\u001b[39m \u001b[43m            \u001b[49m\u001b[43mtags\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtags\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    394\u001b[39m \u001b[43m            \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmetadata\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    395\u001b[39m \u001b[43m            \u001b[49m\u001b[43mrun_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mrun_name\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    396\u001b[39m \u001b[43m            \u001b[49m\u001b[43mrun_id\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpop\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mrun_id\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    397\u001b[39m \u001b[43m            \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    398\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    399\u001b[39m         .generations[\u001b[32m0\u001b[39m][\u001b[32m0\u001b[39m]\n\u001b[32m    400\u001b[39m         .text\n\u001b[32m    401\u001b[39m     )\n",
            "\u001b[36mFile \u001b[39m\u001b[32md:\\Coding\\langchain_Models\\venv\\Lib\\site-packages\\langchain_core\\language_models\\llms.py:766\u001b[39m, in \u001b[36mBaseLLM.generate_prompt\u001b[39m\u001b[34m(self, prompts, stop, callbacks, **kwargs)\u001b[39m\n\u001b[32m    757\u001b[39m \u001b[38;5;129m@override\u001b[39m\n\u001b[32m    758\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mgenerate_prompt\u001b[39m(\n\u001b[32m    759\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    763\u001b[39m     **kwargs: Any,\n\u001b[32m    764\u001b[39m ) -> LLMResult:\n\u001b[32m    765\u001b[39m     prompt_strings = [p.to_string() \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m prompts]\n\u001b[32m--> \u001b[39m\u001b[32m766\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt_strings\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32md:\\Coding\\langchain_Models\\venv\\Lib\\site-packages\\langchain_core\\language_models\\llms.py:971\u001b[39m, in \u001b[36mBaseLLM.generate\u001b[39m\u001b[34m(self, prompts, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\u001b[39m\n\u001b[32m    956\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\u001b[38;5;28mself\u001b[39m.cache \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m get_llm_cache() \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m.cache \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m:\n\u001b[32m    957\u001b[39m     run_managers = [\n\u001b[32m    958\u001b[39m         callback_manager.on_llm_start(\n\u001b[32m    959\u001b[39m             \u001b[38;5;28mself\u001b[39m._serialized,\n\u001b[32m   (...)\u001b[39m\u001b[32m    969\u001b[39m         )\n\u001b[32m    970\u001b[39m     ]\n\u001b[32m--> \u001b[39m\u001b[32m971\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_generate_helper\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    972\u001b[39m \u001b[43m        \u001b[49m\u001b[43mprompts\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    973\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    974\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrun_managers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    975\u001b[39m \u001b[43m        \u001b[49m\u001b[43mnew_arg_supported\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mbool\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mnew_arg_supported\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    976\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    977\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    978\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(missing_prompts) > \u001b[32m0\u001b[39m:\n\u001b[32m    979\u001b[39m     run_managers = [\n\u001b[32m    980\u001b[39m         callback_managers[idx].on_llm_start(\n\u001b[32m    981\u001b[39m             \u001b[38;5;28mself\u001b[39m._serialized,\n\u001b[32m   (...)\u001b[39m\u001b[32m    988\u001b[39m         \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m missing_prompt_idxs\n\u001b[32m    989\u001b[39m     ]\n",
            "\u001b[36mFile \u001b[39m\u001b[32md:\\Coding\\langchain_Models\\venv\\Lib\\site-packages\\langchain_core\\language_models\\llms.py:792\u001b[39m, in \u001b[36mBaseLLM._generate_helper\u001b[39m\u001b[34m(self, prompts, stop, run_managers, new_arg_supported, **kwargs)\u001b[39m\n\u001b[32m    781\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_generate_helper\u001b[39m(\n\u001b[32m    782\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    783\u001b[39m     prompts: \u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mstr\u001b[39m],\n\u001b[32m   (...)\u001b[39m\u001b[32m    788\u001b[39m     **kwargs: Any,\n\u001b[32m    789\u001b[39m ) -> LLMResult:\n\u001b[32m    790\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    791\u001b[39m         output = (\n\u001b[32m--> \u001b[39m\u001b[32m792\u001b[39m             \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_generate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    793\u001b[39m \u001b[43m                \u001b[49m\u001b[43mprompts\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    794\u001b[39m \u001b[43m                \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    795\u001b[39m \u001b[43m                \u001b[49m\u001b[38;5;66;43;03m# TODO: support multiple run managers\u001b[39;49;00m\n\u001b[32m    796\u001b[39m \u001b[43m                \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrun_managers\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrun_managers\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    797\u001b[39m \u001b[43m                \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    798\u001b[39m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    799\u001b[39m             \u001b[38;5;28;01mif\u001b[39;00m new_arg_supported\n\u001b[32m    800\u001b[39m             \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m._generate(prompts, stop=stop)\n\u001b[32m    801\u001b[39m         )\n\u001b[32m    802\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    803\u001b[39m         \u001b[38;5;28;01mfor\u001b[39;00m run_manager \u001b[38;5;129;01min\u001b[39;00m run_managers:\n",
            "\u001b[36mFile \u001b[39m\u001b[32md:\\Coding\\langchain_Models\\venv\\Lib\\site-packages\\langchain_core\\language_models\\llms.py:1545\u001b[39m, in \u001b[36mLLM._generate\u001b[39m\u001b[34m(self, prompts, stop, run_manager, **kwargs)\u001b[39m\n\u001b[32m   1542\u001b[39m new_arg_supported = inspect.signature(\u001b[38;5;28mself\u001b[39m._call).parameters.get(\u001b[33m\"\u001b[39m\u001b[33mrun_manager\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m   1543\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m prompt \u001b[38;5;129;01min\u001b[39;00m prompts:\n\u001b[32m   1544\u001b[39m     text = (\n\u001b[32m-> \u001b[39m\u001b[32m1545\u001b[39m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1546\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m new_arg_supported\n\u001b[32m   1547\u001b[39m         \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m._call(prompt, stop=stop, **kwargs)\n\u001b[32m   1548\u001b[39m     )\n\u001b[32m   1549\u001b[39m     generations.append([Generation(text=text)])\n\u001b[32m   1550\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m LLMResult(generations=generations)\n",
            "\u001b[36mFile \u001b[39m\u001b[32md:\\Coding\\langchain_Models\\venv\\Lib\\site-packages\\langchain_huggingface\\llms\\huggingface_endpoint.py:318\u001b[39m, in \u001b[36mHuggingFaceEndpoint._call\u001b[39m\u001b[34m(self, prompt, stop, run_manager, **kwargs)\u001b[39m\n\u001b[32m    315\u001b[39m         completion += chunk.text\n\u001b[32m    316\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m completion\n\u001b[32m--> \u001b[39m\u001b[32m318\u001b[39m response_text = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mclient\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtext_generation\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    319\u001b[39m \u001b[43m    \u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    320\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    321\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43minvocation_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    322\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    324\u001b[39m \u001b[38;5;66;03m# Maybe the generation has stopped at one of the stop sequences:\u001b[39;00m\n\u001b[32m    325\u001b[39m \u001b[38;5;66;03m# then we remove this stop sequence from the end of the generated text\u001b[39;00m\n\u001b[32m    326\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m stop_seq \u001b[38;5;129;01min\u001b[39;00m invocation_params[\u001b[33m\"\u001b[39m\u001b[33mstop\u001b[39m\u001b[33m\"\u001b[39m]:\n",
            "\u001b[36mFile \u001b[39m\u001b[32md:\\Coding\\langchain_Models\\venv\\Lib\\site-packages\\huggingface_hub\\inference\\_client.py:2376\u001b[39m, in \u001b[36mInferenceClient.text_generation\u001b[39m\u001b[34m(self, prompt, details, stream, model, adapter_id, best_of, decoder_input_details, do_sample, frequency_penalty, grammar, max_new_tokens, repetition_penalty, return_full_text, seed, stop, stop_sequences, temperature, top_k, top_n_tokens, top_p, truncate, typical_p, watermark)\u001b[39m\n\u001b[32m   2374\u001b[39m model_id = model \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m.model\n\u001b[32m   2375\u001b[39m provider_helper = get_provider_helper(\u001b[38;5;28mself\u001b[39m.provider, task=\u001b[33m\"\u001b[39m\u001b[33mtext-generation\u001b[39m\u001b[33m\"\u001b[39m, model=model_id)\n\u001b[32m-> \u001b[39m\u001b[32m2376\u001b[39m request_parameters = \u001b[43mprovider_helper\u001b[49m\u001b[43m.\u001b[49m\u001b[43mprepare_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2377\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2378\u001b[39m \u001b[43m    \u001b[49m\u001b[43mparameters\u001b[49m\u001b[43m=\u001b[49m\u001b[43mparameters\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2379\u001b[39m \u001b[43m    \u001b[49m\u001b[43mextra_payload\u001b[49m\u001b[43m=\u001b[49m\u001b[43m{\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstream\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2380\u001b[39m \u001b[43m    \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2381\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2382\u001b[39m \u001b[43m    \u001b[49m\u001b[43mapi_key\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2383\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2385\u001b[39m \u001b[38;5;66;03m# Handle errors separately for more precise error messages\u001b[39;00m\n\u001b[32m   2386\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
            "\u001b[36mFile \u001b[39m\u001b[32md:\\Coding\\langchain_Models\\venv\\Lib\\site-packages\\huggingface_hub\\inference\\_providers\\_common.py:90\u001b[39m, in \u001b[36mTaskProviderHelper.prepare_request\u001b[39m\u001b[34m(self, inputs, parameters, headers, model, api_key, extra_payload)\u001b[39m\n\u001b[32m     87\u001b[39m api_key = \u001b[38;5;28mself\u001b[39m._prepare_api_key(api_key)\n\u001b[32m     89\u001b[39m \u001b[38;5;66;03m# mapped model from HF model ID\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m90\u001b[39m provider_mapping_info = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_prepare_mapping_info\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     92\u001b[39m \u001b[38;5;66;03m# default HF headers + user headers (to customize in subclasses)\u001b[39;00m\n\u001b[32m     93\u001b[39m headers = \u001b[38;5;28mself\u001b[39m._prepare_headers(headers, api_key)\n",
            "\u001b[36mFile \u001b[39m\u001b[32md:\\Coding\\langchain_Models\\venv\\Lib\\site-packages\\huggingface_hub\\inference\\_providers\\_common.py:159\u001b[39m, in \u001b[36mTaskProviderHelper._prepare_mapping_info\u001b[39m\u001b[34m(self, model)\u001b[39m\n\u001b[32m    156\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mModel \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m is not supported by provider \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m.provider\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    158\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m provider_mapping.task != \u001b[38;5;28mself\u001b[39m.task:\n\u001b[32m--> \u001b[39m\u001b[32m159\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    160\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mModel \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m is not supported for task \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m.task\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m and provider \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m.provider\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m. \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    161\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mSupported task: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mprovider_mapping.task\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    162\u001b[39m     )\n\u001b[32m    164\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m provider_mapping.status == \u001b[33m\"\u001b[39m\u001b[33mstaging\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m    165\u001b[39m     logger.warning(\n\u001b[32m    166\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mModel \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m is in staging mode for provider \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m.provider\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m. Meant for test purposes only.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    167\u001b[39m     )\n",
            "\u001b[31mValueError\u001b[39m: Model google/gemma-2-2b-it is not supported for task text-generation and provider nebius. Supported task: conversational."
          ]
        }
      ],
      "source": [
        "# Query the retriever\n",
        "query = \"What is photosynthesis?\"\n",
        "compressed_results = compression_retriever.invoke(query)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SyIUYw035g0c",
        "outputId": "99a654d5-9239-4912-fd3e-4130c00b115a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "--- Result 1 ---\n",
            "Photosynthesis is the process by which green plants convert sunlight into energy.\n",
            "\n",
            "--- Result 2 ---\n",
            "The chlorophyll in plant cells captures sunlight during photosynthesis.\n"
          ]
        }
      ],
      "source": [
        "for i, doc in enumerate(compressed_results):\n",
        "    print(f\"\\n--- Result {i+1} ---\")\n",
        "    print(doc.page_content)\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
